{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R88isgIyybKN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tt-H4POZ-eoY"
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "def row_col_to_seq(row_col, num_cols):  #Converts state number to row_column format\n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "def seq_to_col_row(seq, num_cols): #Converts row_column format to state number\n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "\n",
    "          arr = self.P[next_state, :, 3]\n",
    "          next_next = np.where(arr == np.amax(arr))\n",
    "          next_next = next_next[0][0]\n",
    "          return next_next, self.R[next_next]\n",
    "        else:\n",
    "          return next_state, self.R[next_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4SDw_hA13bXa"
   },
   "outputs": [],
   "source": [
    "action_space = [0, 1, 2, 3]   #[UP, DOWN, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k_dFMjNuiGq"
   },
   "source": [
    "### Exploration Strategies\n",
    " 1. Epsilon - greedy\n",
    " 2. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y-JlUkI_4ocR"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon - greedy\n",
    "def choose_action_epsilon_greedy(Q, state, epsilon): #state must be an array of riw_index and column_index\n",
    "  if not Q[state[0], state[1]].any() or rg.rand() < epsilon:\n",
    "    return rg.choice(action_space, size = (1, ))[0]\n",
    "  else:\n",
    "    return np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(Q, state, tau):\n",
    "  p = [None for _ in range(len(action_space))]\n",
    "  for i in range(len(action_space)):\n",
    "    p[i] = min(np.exp((Q[state[0], state[1], i]) / tau),1000)\n",
    "  p /= np.sum(p)\n",
    "  return rg.choice(action_space, size = (1, ), p = p)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsZP1Xqn8lUu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h7pXb4t4sXE"
   },
   "source": [
    "### SARSA Algorithm Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(gw, Q, tau0, epsilon0, alpha0, gamma0, episodes = 10000, choose_action = choose_action_softmax):\n",
    "\n",
    "  episode_rewards = np.zeros(episodes)\n",
    "  steps_to_completion = np.zeros(episodes)\n",
    "  state_visit_count = np.zeros((gw.num_rows, gw.num_cols))\n",
    "\n",
    "  parameter = tau0\n",
    "  alpha = alpha0\n",
    "  gamma = gamma0\n",
    "  if choose_action == choose_action_epsilon_greedy:\n",
    "    parameter = epsilon0\n",
    "\n",
    "  for ep in tqdm(range(episodes)):\n",
    "    tot_reward, steps = 0, 0\n",
    "\n",
    "    state = gw.reset()\n",
    "    state_rc = seq_to_col_row(state, gw.num_cols)\n",
    "    action = choose_action(Q, state_rc[0], parameter)\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "      if any(np.array_equal(state_rc[0], state) for state in gw.goal_states) or steps >= 100:\n",
    "        done = True\n",
    "        break\n",
    "\n",
    "      next_state, reward = gw.step(state, action)\n",
    "      next_state_rc = seq_to_col_row(next_state, gw.num_cols)\n",
    "\n",
    "      next_action = choose_action(Q, next_state_rc[0], parameter)\n",
    "      Q[state_rc[0][0], state_rc[0][1],  action] += alpha * (reward + gamma * Q[next_state_rc[0][0], next_state_rc[0][1], next_action] - Q[state_rc[0][0], state_rc[0][1],  action])\n",
    "\n",
    "      state_visit_count[state_rc[0][0], state_rc[0][1]] += 1\n",
    "      tot_reward += reward\n",
    "      steps += 1\n",
    "\n",
    "      state, state_rc, action = next_state, next_state_rc, next_action\n",
    "\n",
    "\n",
    "\n",
    "    episode_rewards[ep] = tot_reward\n",
    "    steps_to_completion[ep] = steps\n",
    "  return Q, episode_rewards, steps_to_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5o7lf26wGp-5"
   },
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],              #All obstruction states\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])                           #All bad states\n",
    "restart_states = np.array([[3,7],[8,2]])                                         #All restart states\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BSLT1peRGsDe"
   },
   "outputs": [],
   "source": [
    "start_state = np.array([[0, 4]])\n",
    "# create model\n",
    "gw = GridWorld(num_rows = num_rows, num_cols = num_cols, start_state = start_state, goal_states = goal_states, wind = False)\n",
    "gw.add_obstructions(obstructed_states = obstructions, bad_states = bad_states, restart_states = restart_states)\n",
    "gw.add_rewards(step_reward = - 1, goal_reward = 10, bad_state_reward = - 6, restart_state_reward = - 100)\n",
    "gw.add_transition_probability(p_good_transition = 0.7, bias = 0.5)\n",
    "env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGWWNLaliRMN",
    "outputId": "2f0b710b-5c79-45c2-a157-0127c4b87cbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/5000 [00:00<?, ?it/s]C:\\Users\\kanni\\AppData\\Local\\Temp\\ipykernel_4212\\2297487967.py:180: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(self.start_state_seq)\n",
      "C:\\Users\\kanni\\AppData\\Local\\Temp\\ipykernel_4212\\12305718.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[state_rc[0][0], state_rc[0][1],  action] += alpha * (reward + gamma * Q[next_state_rc[0][0], next_state_rc[0][1], next_action] - Q[state_rc[0][0], state_rc[0][1],  action])\n",
      "C:\\Users\\kanni\\AppData\\Local\\Temp\\ipykernel_4212\\12305718.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  episode_rewards[ep] = tot_reward\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [03:05<00:00, 26.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:07<00:00, 39.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:41<00:00, 49.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:24<00:00, 59.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:02<00:00, 40.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:38<00:00, 50.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:46<00:00, 107.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:03<00:00, 40.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:03<00:00, 40.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:39<00:00, 125.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:13<00:00, 37.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:50<00:00, 45.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:37<00:00, 131.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:20<00:00, 35.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:17<00:00, 36.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:54<00:00, 43.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:19<00:00, 35.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:14<00:00, 37.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:10<00:00, 70.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:19<00:00, 35.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:11<00:00, 38.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:02<00:00, 79.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:17<00:00, 36.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:03<00:00, 40.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:09<00:00, 72.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:59<00:00, 27.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:47<00:00, 29.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:24<00:00, 34.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:48<00:00, 29.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:33<00:00, 32.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:43<00:00, 48.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:40<00:00, 31.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:34<00:00, 32.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:32<00:00, 54.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:45<00:00, 30.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:33<00:00, 32.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:23<00:00, 60.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:34<00:00, 32.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:59<00:00, 27.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:42<00:00, 30.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:38<00:00, 31.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:35<00:00, 32.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:09<00:00, 38.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:13<00:00, 37.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:54<00:00, 43.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:23<00:00, 59.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:42<00:00, 49.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:25<00:00, 58.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:07<00:00, 74.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.2 1.0 -22.842\n"
     ]
    }
   ],
   "source": [
    "start_state = np.array([[0, 4]])\n",
    "alpha_list = [0.01, 0.05, 0.1, 0.2]\n",
    "gamma_list = [0.8, 0.9, 1.0]\n",
    "tau_list = [1.0, 2.0, 4.0, 10.0]\n",
    "epsilon_list = [0.01, 0.05, 0.1]\n",
    "\n",
    "store_tau = []\n",
    "#store_epsilon = []\n",
    "\n",
    "#For Soft Max\n",
    "best_tau, best_tau_alpha, best_tau_gamma = tau_list[0], alpha_list[0], gamma_list[0]\n",
    "\n",
    "Q = np.zeros((gw.num_rows, gw.num_cols, len(action_space)))\n",
    "\n",
    "Q, rewards, steps = sarsa(gw, Q, best_tau, epsilon_list[0], best_tau_alpha, best_tau_gamma, episodes = 5000)\n",
    "best_tau_reward = np.mean(rewards)\n",
    "for tau in tau_list:\n",
    "  for alpha in alpha_list:\n",
    "    for gamma in gamma_list:\n",
    "      Q = np.zeros((gw.num_rows, gw.num_cols, len(action_space)))\n",
    "      Q, rewards, steps = sarsa(gw, Q, tau, epsilon_list[0], alpha, gamma, episodes = 5000)\n",
    "      store_tau.append({\"tau\" : tau, \"alpha\" : alpha, \"gamma\" : gamma, \"reward\" : np.mean(rewards)})\n",
    "      if np.mean(rewards) > best_tau_reward:\n",
    "        best_tau = tau\n",
    "        best_tau_reward = np.mean(rewards)\n",
    "        best_tau_alpha = alpha\n",
    "        best_tau_gamma = gamma\n",
    "          \n",
    "print(best_tau, best_tau_alpha, best_tau_gamma, best_tau_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2EmPcxFmgQBH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/5000 [00:00<?, ?it/s]C:\\Users\\kanni\\AppData\\Local\\Temp\\ipykernel_4212\\2297487967.py:180: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(self.start_state_seq)\n",
      "C:\\Users\\kanni\\AppData\\Local\\Temp\\ipykernel_4212\\12305718.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[state_rc[0][0], state_rc[0][1],  action] += alpha * (reward + gamma * Q[next_state_rc[0][0], next_state_rc[0][1], next_action] - Q[state_rc[0][0], state_rc[0][1],  action])\n",
      "C:\\Users\\kanni\\AppData\\Local\\Temp\\ipykernel_4212\\12305718.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  episode_rewards[ep] = tot_reward\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:25<00:00, 196.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:24<00:00, 200.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:22<00:00, 222.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:20<00:00, 244.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 386.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:13<00:00, 380.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 390.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:44<00:00, 113.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:11<00:00, 450.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 479.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:42<00:00, 116.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 480.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:11<00:00, 452.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:27<00:00, 182.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:22<00:00, 225.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:23<00:00, 215.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:14<00:00, 341.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 415.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:11<00:00, 439.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:32<00:00, 153.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 460.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 469.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:35<00:00, 140.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 481.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 486.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:27<00:00, 179.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:21<00:00, 230.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:22<00:00, 220.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:29<00:00, 167.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 389.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 392.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:39<00:00, 125.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:10<00:00, 474.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:11<00:00, 447.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:37<00:00, 132.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:12<00:00, 413.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:11<00:00, 423.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.2 1.0 -17.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_state = np.array([[0, 4]])\n",
    "alpha_list = [0.01, 0.05, 0.1, 0.2]\n",
    "gamma_list = [0.8, 0.9, 1.0]\n",
    "tau_list = [1.0, 2.0, 4.0, 10.0]\n",
    "epsilon_list = [0.01, 0.05, 0.1]\n",
    "\n",
    "#store_tau = []\n",
    "store_epsilon = []\n",
    "\n",
    "# For Epsilon Greedy\n",
    "best_epsilon, best_epsilon_alpha, best_epsilon_gamma = epsilon_list[0], alpha_list[0], gamma_list[0]\n",
    "\n",
    "Q = np.zeros((gw.num_rows, gw.num_cols, len(action_space)))\n",
    "\n",
    "Q, rewards, steps = sarsa(gw, Q, tau_list[0], best_epsilon, best_epsilon_alpha, best_epsilon_gamma, episodes = 5000, choose_action = choose_action_epsilon_greedy)\n",
    "best_epsilon_reward = np.mean(rewards)\n",
    "for epsilon in epsilon_list:\n",
    "  for alpha in alpha_list:\n",
    "    for gamma in gamma_list:\n",
    "      Q = np.zeros((gw.num_rows, gw.num_cols, len(action_space)))\n",
    "      Q, rewards, steps = sarsa(gw, Q, tau_list[0], epsilon, alpha, gamma, episodes = 5000, choose_action = choose_action_epsilon_greedy)\n",
    "      store_epsilon.append({\"epsilon\" : epsilon, \"alpha\" : alpha, \"gamma\" : gamma, \"reward\" : np.mean(rewards)})\n",
    "      if np.mean(rewards) > best_epsilon_reward:\n",
    "        best_epsilon = epsilon\n",
    "        best_epsilon_reward = np.mean(rewards)\n",
    "        best_epsilon_alpha = alpha\n",
    "        best_epsilon_gamma = gamma\n",
    "          \n",
    "print(best_epsilon, best_epsilon_alpha, best_epsilon_gamma, best_epsilon_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tau': 1.0, 'alpha': 0.01, 'gamma': 0.8, 'reward': -101.786}, {'tau': 1.0, 'alpha': 0.01, 'gamma': 0.9, 'reward': -94.8986}, {'tau': 1.0, 'alpha': 0.01, 'gamma': 1.0, 'reward': -70.2554}, {'tau': 1.0, 'alpha': 0.05, 'gamma': 0.8, 'reward': -95.1628}, {'tau': 1.0, 'alpha': 0.05, 'gamma': 0.9, 'reward': -78.88}, {'tau': 1.0, 'alpha': 0.05, 'gamma': 1.0, 'reward': -34.0044}, {'tau': 1.0, 'alpha': 0.1, 'gamma': 0.8, 'reward': -93.7704}, {'tau': 1.0, 'alpha': 0.1, 'gamma': 0.9, 'reward': -76.2522}, {'tau': 1.0, 'alpha': 0.1, 'gamma': 1.0, 'reward': -23.4564}, {'tau': 1.0, 'alpha': 0.2, 'gamma': 0.8, 'reward': -93.1136}, {'tau': 1.0, 'alpha': 0.2, 'gamma': 0.9, 'reward': -75.1842}, {'tau': 1.0, 'alpha': 0.2, 'gamma': 1.0, 'reward': -22.842}, {'tau': 2.0, 'alpha': 0.01, 'gamma': 0.8, 'reward': -107.0452}, {'tau': 2.0, 'alpha': 0.01, 'gamma': 0.9, 'reward': -105.5626}, {'tau': 2.0, 'alpha': 0.01, 'gamma': 1.0, 'reward': -91.6798}, {'tau': 2.0, 'alpha': 0.05, 'gamma': 0.8, 'reward': -99.636}, {'tau': 2.0, 'alpha': 0.05, 'gamma': 0.9, 'reward': -94.0054}, {'tau': 2.0, 'alpha': 0.05, 'gamma': 1.0, 'reward': -47.968}, {'tau': 2.0, 'alpha': 0.1, 'gamma': 0.8, 'reward': -98.3372}, {'tau': 2.0, 'alpha': 0.1, 'gamma': 0.9, 'reward': -91.3628}, {'tau': 2.0, 'alpha': 0.1, 'gamma': 1.0, 'reward': -40.2482}, {'tau': 2.0, 'alpha': 0.2, 'gamma': 0.8, 'reward': -97.159}, {'tau': 2.0, 'alpha': 0.2, 'gamma': 0.9, 'reward': -90.5864}, {'tau': 2.0, 'alpha': 0.2, 'gamma': 1.0, 'reward': -34.9878}, {'tau': 4.0, 'alpha': 0.01, 'gamma': 0.8, 'reward': -116.1296}, {'tau': 4.0, 'alpha': 0.01, 'gamma': 0.9, 'reward': -115.4194}, {'tau': 4.0, 'alpha': 0.01, 'gamma': 1.0, 'reward': -114.8554}, {'tau': 4.0, 'alpha': 0.05, 'gamma': 0.8, 'reward': -106.4792}, {'tau': 4.0, 'alpha': 0.05, 'gamma': 0.9, 'reward': -104.4628}, {'tau': 4.0, 'alpha': 0.05, 'gamma': 1.0, 'reward': -69.9052}, {'tau': 4.0, 'alpha': 0.1, 'gamma': 0.8, 'reward': -104.3606}, {'tau': 4.0, 'alpha': 0.1, 'gamma': 0.9, 'reward': -100.7652}, {'tau': 4.0, 'alpha': 0.1, 'gamma': 1.0, 'reward': -60.4878}, {'tau': 4.0, 'alpha': 0.2, 'gamma': 0.8, 'reward': -102.4124}, {'tau': 4.0, 'alpha': 0.2, 'gamma': 0.9, 'reward': -100.082}, {'tau': 4.0, 'alpha': 0.2, 'gamma': 1.0, 'reward': -54.6262}, {'tau': 10.0, 'alpha': 0.01, 'gamma': 0.8, 'reward': -130.5996}, {'tau': 10.0, 'alpha': 0.01, 'gamma': 0.9, 'reward': -130.161}, {'tau': 10.0, 'alpha': 0.01, 'gamma': 1.0, 'reward': -132.9634}, {'tau': 10.0, 'alpha': 0.05, 'gamma': 0.8, 'reward': -123.7494}, {'tau': 10.0, 'alpha': 0.05, 'gamma': 0.9, 'reward': -121.7296}, {'tau': 10.0, 'alpha': 0.05, 'gamma': 1.0, 'reward': -105.362}, {'tau': 10.0, 'alpha': 0.1, 'gamma': 0.8, 'reward': -122.3968}, {'tau': 10.0, 'alpha': 0.1, 'gamma': 0.9, 'reward': -118.8208}, {'tau': 10.0, 'alpha': 0.1, 'gamma': 1.0, 'reward': -93.0316}, {'tau': 10.0, 'alpha': 0.2, 'gamma': 0.8, 'reward': -118.6858}, {'tau': 10.0, 'alpha': 0.2, 'gamma': 0.9, 'reward': -116.1906}, {'tau': 10.0, 'alpha': 0.2, 'gamma': 1.0, 'reward': -86.4492}]\n"
     ]
    }
   ],
   "source": [
    "print(store_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'epsilon': 0.01, 'alpha': 0.01, 'gamma': 0.8, 'reward': -54.3526}, {'epsilon': 0.01, 'alpha': 0.01, 'gamma': 0.9, 'reward': -47.5354}, {'epsilon': 0.01, 'alpha': 0.01, 'gamma': 1.0, 'reward': -44.348}, {'epsilon': 0.01, 'alpha': 0.05, 'gamma': 0.8, 'reward': -26.6506}, {'epsilon': 0.01, 'alpha': 0.05, 'gamma': 0.9, 'reward': -22.6116}, {'epsilon': 0.01, 'alpha': 0.05, 'gamma': 1.0, 'reward': -21.746}, {'epsilon': 0.01, 'alpha': 0.1, 'gamma': 0.8, 'reward': -99.1406}, {'epsilon': 0.01, 'alpha': 0.1, 'gamma': 0.9, 'reward': -19.6028}, {'epsilon': 0.01, 'alpha': 0.1, 'gamma': 1.0, 'reward': -18.9812}, {'epsilon': 0.01, 'alpha': 0.2, 'gamma': 0.8, 'reward': -97.741}, {'epsilon': 0.01, 'alpha': 0.2, 'gamma': 0.9, 'reward': -18.831}, {'epsilon': 0.01, 'alpha': 0.2, 'gamma': 1.0, 'reward': -17.9806}, {'epsilon': 0.05, 'alpha': 0.01, 'gamma': 0.8, 'reward': -55.8566}, {'epsilon': 0.05, 'alpha': 0.01, 'gamma': 0.9, 'reward': -47.8514}, {'epsilon': 0.05, 'alpha': 0.01, 'gamma': 1.0, 'reward': -46.3848}, {'epsilon': 0.05, 'alpha': 0.05, 'gamma': 0.8, 'reward': -29.1626}, {'epsilon': 0.05, 'alpha': 0.05, 'gamma': 0.9, 'reward': -23.8568}, {'epsilon': 0.05, 'alpha': 0.05, 'gamma': 1.0, 'reward': -23.4416}, {'epsilon': 0.05, 'alpha': 0.1, 'gamma': 0.8, 'reward': -78.866}, {'epsilon': 0.05, 'alpha': 0.1, 'gamma': 0.9, 'reward': -20.9572}, {'epsilon': 0.05, 'alpha': 0.1, 'gamma': 1.0, 'reward': -20.7496}, {'epsilon': 0.05, 'alpha': 0.2, 'gamma': 0.8, 'reward': -91.8086}, {'epsilon': 0.05, 'alpha': 0.2, 'gamma': 0.9, 'reward': -20.911}, {'epsilon': 0.05, 'alpha': 0.2, 'gamma': 1.0, 'reward': -19.8752}, {'epsilon': 0.1, 'alpha': 0.01, 'gamma': 0.8, 'reward': -59.5242}, {'epsilon': 0.1, 'alpha': 0.01, 'gamma': 0.9, 'reward': -48.7984}, {'epsilon': 0.1, 'alpha': 0.01, 'gamma': 1.0, 'reward': -52.0172}, {'epsilon': 0.1, 'alpha': 0.05, 'gamma': 0.8, 'reward': -66.7532}, {'epsilon': 0.1, 'alpha': 0.05, 'gamma': 0.9, 'reward': -25.6646}, {'epsilon': 0.1, 'alpha': 0.05, 'gamma': 1.0, 'reward': -25.832}, {'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.8, 'reward': -99.5894}, {'epsilon': 0.1, 'alpha': 0.1, 'gamma': 0.9, 'reward': -22.9348}, {'epsilon': 0.1, 'alpha': 0.1, 'gamma': 1.0, 'reward': -22.7262}, {'epsilon': 0.1, 'alpha': 0.2, 'gamma': 0.8, 'reward': -92.4536}, {'epsilon': 0.1, 'alpha': 0.2, 'gamma': 0.9, 'reward': -22.895}, {'epsilon': 0.1, 'alpha': 0.2, 'gamma': 1.0, 'reward': -22.0672}]\n"
     ]
    }
   ],
   "source": [
    "print(store_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_store = [None for _ in range(len(store_epsilon))]\n",
    "alpha_epsilon_store = [None for _ in range(len(store_epsilon))]\n",
    "gamma_epsilon_store = [None for _ in range(len(store_epsilon))]\n",
    "reward_epsilon_store = [None for _ in range(len(store_epsilon))]\n",
    "for i in range(len(store_epsilon)):\n",
    "    epsilon_store[i] = store_epsilon[i]['epsilon']\n",
    "    alpha_epsilon_store[i] = store_epsilon[i]['alpha']\n",
    "    gamma_epsilon_store[i] = store_epsilon[i]['gamma']\n",
    "    reward_epsilon_store[i] = store_epsilon[i]['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_2_epsilon = pd.DataFrame({'Epsilon' : epsilon_store, 'Alpha' : alpha_epsilon_store, 'Gamma' : gamma_epsilon_store, 'Reward' : reward_epsilon_store})\n",
    "EXP_2_epsilon.to_csv(\"EXP_2_epsilon.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_store = [None for _ in range(len(store_tau))]\n",
    "alpha_tau_store = [None for _ in range(len(store_tau))]\n",
    "gamma_tau_store = [None for _ in range(len(store_tau))]\n",
    "reward_tau_store = [None for _ in range(len(store_tau))]\n",
    "for i in range(len(store_tau)):\n",
    "    tau_store[i] = store_tau[i]['tau']\n",
    "    alpha_tau_store[i] = store_tau[i]['alpha']\n",
    "    gamma_tau_store[i] = store_tau[i]['gamma']\n",
    "    reward_tau_store[i] = store_tau[i]['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_2_tau = pd.DataFrame({'Tau' : tau_store, 'Alpha' : alpha_tau_store, 'Gamma' : gamma_tau_store, 'Reward' : reward_tau_store})\n",
    "EXP_2_tau.to_csv(\"EXP_2_tau.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
